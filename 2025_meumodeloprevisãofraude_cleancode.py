# -*- coding: utf-8 -*-
"""2025_MeuModeloPrevis√£oFraude_CleanCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11OfTIl94kMaFZmTuVhyeOZyPjutUOiPT
"""

import pandas as pd
import warnings
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import numpy as np
from xgboost import XGBClassifier
# from imblearn import under_sampling, over_sampling
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score
from sklearn.model_selection import cross_val_score

warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.options.display.float_format = '{:.2f}'.format # configura√ß√£o para n√£o ficar em nota√ß√£o cient√≠fica

# Carregar os dados
file_path = "C:/Users/giuli/Documents/Analytics/An√°lise Fraude/dados_coletados10k.csv"
df = pd.read_csv(file_path)

# Exibir as primeiras linhas do dataframe para inspe√ß√£o inicial
df.head()

# Verificar informa√ß√µes gerais do dataset
df.info()

# Verificar a presen√ßa de valores ausentes
missing_values = df.isnull().sum()
missing_values_percentage = (missing_values / len(df)) * 100

# Criar um dataframe com a contagem e percentual de valores ausentes
missing_df = pd.DataFrame({
    "[Coluna]": missing_values.index,
    "[Valores Ausentes]": missing_values.values,
    "[% Valores Ausentes]": missing_values_percentage.values
})

missing_df

# Verificar a distribui√ß√£o das vari√°veis num√©ricas
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
numeric_summary = df[numeric_columns].describe()

numeric_summary

# Verificar a distribui√ß√£o das vari√°veis categ√≥ricas
categorical_columns = df.select_dtypes(include=['object']).columns

categorical_summary = {}
for col in categorical_columns:
    categorical_summary[col] = df[col].value_counts()

dataframe=pd.DataFrame(categorical_summary)

dataframe

# AP√ìS ANALISE INICIAL QUE REALIZAMOS ACIMA, ENTENDEMOSO QUE ALGUMAS VARI√ÅVEIS N√ÉO POSSUEM RELEVANCIA.

#  Contrato --> Essa vari√°vel √© a identifica√ß√£o de cada cliente
#  Data_Contratacao, VL_Patrimonio, Possui_Patrimonio, Escolaridade --> Essas n√£o ir√£o ter relevancia no modelo

# Remover as colunas especificadas
columns_to_drop = ["Contrato", "Data_Contratacao", "Escolaridade", "Possui_Patrimonio", "VL_Patrimonio"]
df = df.drop(columns=columns_to_drop)

df.head()

# Substituir valores ausentes em QT_Dias_Atraso pela mediana, pois √© menos sens√≠vel a outliers
df["QT_Dias_Atraso"] = df["QT_Dias_Atraso"].fillna(df["QT_Dias_Atraso"].median())
df.head()

# Vamos constatar que realmente n√£o h√° valores nulos
df.isnull().sum()

#Aqui vamos realizar a engenharia de features - nada mais √© do que a cria√ß√£o de novas colunas baseadas nos dados que j√° existem na base. Essa cria√ß√£o pode ser de m√©tricas novas, realizando c√°lculos com as colunas existentes ou agrupamento de vari√°veis categ√≥ricas etc. \"""

# Criando as novas colunas no dataframe
df_transformed = df

# 1Ô∏è‚É£ Rela√ß√£o Renda/Empr√©stimo
df_transformed["Rela√ß√£o_Renda_Emprestimo"] = df_transformed["VL_Emprestimo"] / df_transformed["Valor_Renda"]

# 2Ô∏è‚É£ Rela√ß√£o Parcelas Pagas
df_transformed["Rela√ß√£o_Parcelas_Pagas"] = df_transformed["QT_Total_Parcelas_Pagas"] / df_transformed["Prazo_Emprestimo"]

# 3Ô∏è‚É£ Inadimpl√™ncia
df_transformed["Inadimpl√™ncia"] = df_transformed["QT_Parcelas_Atraso"] / df_transformed["QT_Total_Parcelas_Pagas"]
df_transformed["Inadimpl√™ncia"].fillna(0, inplace=True)  # Evitar NaNs quando n√£o h√° parcelas pagas

# 4Ô∏è‚É£ Dura√ß√£o Restante Proporcional
df_transformed["Dura√ß√£o_Restante_Proporcional"] = df_transformed["Prazo_Restante"] / df_transformed["Prazo_Emprestimo"]

# 5Ô∏è‚É£ Rela√ß√£o Juros/Empr√©stimo
df_transformed["Rela√ß√£o_Juros_Emprestimo"] = df_transformed["Perc_Juros"] * df_transformed["VL_Emprestimo"]

# 6Ô∏è‚É£ Comprometimento da Renda
df_transformed["Comprometimento_Renda"] = df_transformed["Total_Pago"] / df_transformed["Valor_Renda"]

# 7Ô∏è‚É£ Tempo M√©dio de Atraso
df_transformed["Tempo_M√©dio_Atraso"] = df_transformed["QT_Dias_Atraso"] / df_transformed["QT_Parcelas_Atraso"]
df_transformed["Tempo_M√©dio_Atraso"].fillna(0, inplace=True)  # Evitar NaNs quando n√£o h√° atraso

# 8Ô∏è‚É£ Hist√≥rico de Renegocia√ß√£o
df_transformed["Hist√≥rico_Renegocia√ß√£o"] = df_transformed["Qt_Renegociacao"] / df_transformed["QT_Total_Parcelas_Pagas"]
df_transformed["Hist√≥rico_Renegocia√ß√£o"].fillna(0, inplace=True)  # Evitar NaNs quando n√£o h√° renegocia√ß√£o

# 9Ô∏è‚É£ Faixa_Prazo_Restante
df_transformed["Faixa_Prazo_Restante"] = pd.cut(
    df_transformed["Prazo_Restante"],
    bins=[-0.1, 0, 24, 60, 120, float("inf")],  # Incluindo -0.1 para capturar 0 corretamente
    labels=["Quitado", "Curto prazo", "M√©dio prazo", "Longo prazo", "Muito longo prazo"]
)

# üîü Faixa_Salarial
df_transformed["Faixa_Salarial"] = pd.cut(
    df_transformed["Valor_Renda"],
    bins=[0, 2500, 5000, 10000, 20000, float("inf")],
    labels=["Baixa renda", "Classe m√©dia baixa", "Classe m√©dia", "Classe m√©dia alta", "Alta renda"]
)

# 1Ô∏è‚É£1Ô∏è‚É£ Faixa_Prazo_Emprestimo
df_transformed["Faixa_Prazo_Emprestimo"] = pd.cut(
    df_transformed["Prazo_Emprestimo"],
    bins=[0, 24, 60, 120, float("inf")],
    labels=["Curto prazo", "M√©dio prazo", "Longo prazo", "Muito longo prazo"]
)

# 1Ô∏è‚É£2Ô∏è‚É£ Faixa_Etaria
df_transformed["Faixa_Etaria"] = pd.cut(
    df_transformed["Idade"],
    bins=[0, 25, 40, 60, float("inf")],
    labels=["Jovem", "Adulto jovem", "Meia-idade", "Idoso"]
)

# 1Ô∏è‚É£3Ô∏è‚É£ Faixa_Dias_Atraso
df_transformed["Faixa_Dias_Atraso"] = pd.cut(
    df_transformed["QT_Dias_Atraso"],
    bins=[-1, 0, 30, 90, 180, float("inf")],  # -1 para incluir zero
    labels=["Sem atraso", "Atraso leve", "Atraso moderado", "Atraso grave", "Atraso cr√≠tico"]
)

# Vamos constatar que realmente n√£o h√° valores nulos
df_transformed.isnull().sum()

df_transformed.head()

"""
Minha recomenda√ß√£o √© manter as colunas originais por enquanto, pelos seguintes motivos:

1Ô∏è Evitar perda de informa√ß√µes √∫teis
Algumas colunas originais podem conter informa√ß√µes que ainda s√£o relevantes para o modelo, mesmo que tenham sido utilizadas para criar novas features.
Exemplo: "Valor_Renda" foi usada para criar "Rela√ß√£o_Renda_Emprestimo" e "Comprometimento_Renda", mas pode ser √∫til isoladamente para algumas t√©cnicas de modelagem.
2Ô∏è Permitir an√°lise de impacto das novas features
Manter as colunas originais nos permite comparar o desempenho das novas features no modelo.
Podemos fazer testes de import√¢ncia das vari√°veis para verificar quais realmente agregam valor.
3Ô∏è Evitar vi√©s de engenharia prematura
Nem sempre novas features s√£o melhores do que os dados brutos. Algumas combina√ß√µes podem n√£o ter tanto impacto no modelo.
Remover colunas agora pode restringir as possibilidades de ajuste fino na modelagem.
4Ô∏è Impacto nos algoritmos de Machine Learning
Algoritmos como √°rvores de decis√£o e redes neurais podem se beneficiar de m√∫ltiplas representa√ß√µes dos dados.
Por outro lado, algoritmos como regress√£o log√≠stica e SVMs podem sofrer com multicolinearidade se houver redund√¢ncia demais.
Quando remover as colunas originais?
Se ap√≥s uma an√°lise de correla√ß√£o e import√¢ncia das vari√°veis percebermos que algumas colunas originais n√£o agregam valor ao modelo (ou geram multicolinearidade), podemos remov√™-las.

Sugest√£o: Antes de descartar qualquer coluna, rodamos uma an√°lise de feature importance com algoritmos como Random Forest ou XGBoost.

Conclus√£o:
‚û° Por enquanto, mantemos as colunas originais. üî•
‚û° Depois da an√°lise de import√¢ncia das vari√°veis, decidimos quais descartar.
"""

# Remover colunas categ√≥ricas para o modelo inicial
categorical_columns = ["Sexo", "UF_Cliente", "Estado_Civil", "Faixa_Prazo_Restante", "Faixa_Salarial",
                       "Faixa_Prazo_Emprestimo", "Faixa_Etaria", "Faixa_Dias_Atraso"]

# Criar c√≥pia do dataframe para transforma√ß√£o
# df_model = df_transformed

# Transformar colunas categ√≥ricas em valores num√©ricos usando Label Encoding
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    df_transformed[col] = le.fit_transform(df_transformed[col])
    label_encoders[col] = le  # Armazena o encoder caso precise ser revertido depois

# Tratar valores infinitos na coluna Tempo_M√©dio_Atraso
df_transformed["Tempo_M√©dio_Atraso"] = df_transformed["Tempo_M√©dio_Atraso"].replace([np.inf, -np.inf], np.nan)
df_transformed["Tempo_M√©dio_Atraso"] = df_transformed["Tempo_M√©dio_Atraso"].fillna(df_transformed["Tempo_M√©dio_Atraso"].median())

# Vamos constatar que realmente n√£o h√° valores nulos
df_transformed.isnull().sum()

"""Dividir os dados em treino e teste para rodarmos os algoritmos. Nessa etapa ainda n√£o vamos fazer previs√µes, vamos utilizar os algoritmos para nos mostrar quais s√£o as colunas mais relevantes da base de dados."""

# Definir vari√°veis independentes (X) e vari√°vel alvo (y)
X = df_transformed.drop(columns=["Possivel_Fraude"])  # Features
y = LabelEncoder().fit_transform(df_transformed["Possivel_Fraude"])  # Vari√°vel alvo

# Verificar novamente se h√° valores infinitos ou extremamente grandes
problematic_values = X[(X == np.inf) | (X == -np.inf) | (X > np.finfo(np.float32).max)].any()

# Criar um relat√≥rio das colunas problem√°ticas
problematic_values_df = pd.DataFrame(problematic_values, columns=["Possui Problema"])
problematic_values_df = problematic_values_df[problematic_values_df["Possui Problema"]]

problematic_values_df

# Substituir valores infinitos por 0 nas colunas Inadimpl√™ncia e Hist√≥rico_Renegocia√ß√£o
X["Inadimpl√™ncia"] = X["Inadimpl√™ncia"].replace([np.inf, -np.inf], 0)
X["Hist√≥rico_Renegocia√ß√£o"] = X["Hist√≥rico_Renegocia√ß√£o"].replace([np.inf, -np.inf], 0)

# Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Treinar o modelo Random Forest para an√°lise de import√¢ncia das features
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Obter import√¢ncia das features
feature_importances = pd.DataFrame({"Feature": X.columns, "Importance": rf_model.feature_importances_})
feature_importances = feature_importances.sort_values(by="Importance", ascending=False)

feature_importances

# Agora vamos rodar o XGBoost para avaliar tamb√©m as colunas mais relevantes em compara√ß√£o com o random forest.\"""

# Ajustar o modelo XGBoost para execu√ß√£o mais eficiente
xgb_model = XGBClassifier(
    n_estimators=50,  # Reduzindo o n√∫mero de √°rvores
    use_label_encoder=False,
    eval_metric='logloss',
    tree_method='hist',  # M√©todo mais eficiente
    random_state=42
)

# Treinar o modelo XGBoost novamente
xgb_model.fit(X_train, y_train)

# Obter import√¢ncia das features do XGBoost
xgb_feature_importances = pd.DataFrame({"Feature": X.columns, "Importance": xgb_model.feature_importances_})
xgb_feature_importances = xgb_feature_importances.sort_values(by="Importance", ascending=False)

xgb_feature_importances

"""üìä Compara√ß√£o dos Modelos
üîπ Random Forest (Top 5 Features)

1Ô∏è‚É£ QT_Parcelas_Atraso ‚Üí 26.47%

2Ô∏è‚É£ Inadimpl√™ncia ‚Üí 18.39%

3Ô∏è‚É£ QT_Total_Parcelas_Pagas ‚Üí 16.15%

4Ô∏è‚É£ Rela√ß√£o_Parcelas_Pagas ‚Üí 9.34%

5Ô∏è‚É£ Comprometimento_Renda ‚Üí 6.21%

üîπ XGBoost (Top 5 Features)

1Ô∏è‚É£ QT_Parcelas_Atraso ‚Üí 86% ‚úÖ (extremamente relevante para XGBoost)

2Ô∏è‚É£ Qt_Renegociacao ‚Üí 2%

3Ô∏è‚É£ Tempo_M√©dio_Atraso ‚Üí 2%

4Ô∏è‚É£ Comprometimento_Renda ‚Üí 1%

5Ô∏è‚É£ QT_Dias_Atraso ‚Üí 1%

Agora precisamos decidir quais features devemos manter e quais podemos remover. Com base nos resultados, minha sugest√£o:

‚úÖ Manter as features mais relevantes nos dois modelos:

1.   QT_Parcelas_Atraso (Ambos os modelos)
2.   QT_Total_Parcelas_Pagas (Random Forest)
3.   Inadimpl√™ncia (Random Forest)
4.   Comprometimento_Renda (Ambos os modelos)
5.   Tempo_M√©dio_Atraso (XGBoost)
6.   QT_Dias_Atraso (XGBoost)

‚ùå Remover as features que tiveram import√¢ncia zero no XGBoost e baixa relev√¢ncia no Random Forest:

Idade, Sexo, Estado_Civil
UF_Cliente
Prazo_Emprestimo, Prazo_Restante
VL_Emprestimo, VL_Emprestimo_ComJuros
Faixa_Etaria, Faixa_Salarial, Faixa_Prazo_Restante, Faixa_Prazo_Emprestimo, Faixa_Dias_Atraso
Rela√ß√£o_Renda_Emprestimo, Rela√ß√£o_Juros_Emprestimo, Hist√≥rico_Renegocia√ß√£o, Dura√ß√£o_Restante_Proporcional

Devemos normalizar os dados antes da an√°lise de import√¢ncia das features?
Para √Årvores de Decis√£o (Random Forest e XGBoost)

‚ùå N√£o √© necess√°rio

Esses algoritmos s√£o baseados em divis√µes sucessivas dos dados e n√£o s√£o sens√≠veis a escalas.
Como eles usam apenas ordem relativa dos valores, n√£o √© necess√°rio normalizar antes da an√°lise de feature importance.
Para Modelos Baseados em Dist√¢ncias (Regress√£o Log√≠stica, SVM, Redes Neurais, KNN)

‚úÖ √â necess√°rio

Esses modelos usam c√°lculos baseados em dist√¢ncias euclidianas, que podem ser distorcidos se os dados estiverem em escalas diferentes.
Exemplo: Se "Valor_Renda" varia entre R$1.000 e R$50.000, mas "Inadimpl√™ncia" est√° entre 0 e 1, o modelo pode atribuir mais peso √† renda simplesmente porque tem n√∫meros maiores.
"""

# Lista das colunas a serem mantidas com base na an√°lise de import√¢ncia das features
columns_to_keep = [
    "QT_Parcelas_Atraso", "QT_Total_Parcelas_Pagas", "Inadimpl√™ncia",
    "Comprometimento_Renda", "Tempo_M√©dio_Atraso", "QT_Dias_Atraso"
]

# Criar um novo dataframe apenas com as colunas relevantes
df_transformed_reduced = df_transformed[columns_to_keep + ["Possivel_Fraude"]]  # Mantendo a vari√°vel alvo

df_transformed_reduced.head()

# Verificar a presen√ßa de valores NaN e infinitos no dataset reduzido
nan_counts = df_transformed_reduced.isna().sum()
inf_counts = (df_transformed_reduced == np.inf).sum()

# Criar um relat√≥rio dos problemas encontrados
validation_report = pd.DataFrame({
    "Coluna": df_transformed_reduced.columns,
    "Valores NaN": nan_counts.values,
    "Valores Infinitos": inf_counts.values
})

validation_report

# Substituir valores infinitos por 0 em 'Inadimpl√™ncia'
df_transformed_reduced["Inadimpl√™ncia"] = df_transformed_reduced["Inadimpl√™ncia"].replace([np.inf, -np.inf], 0)

# Verificar a presen√ßa de valores NaN e infinitos no dataset reduzido
nan_counts = df_transformed_reduced.isna().sum()
inf_counts = (df_transformed_reduced == np.inf).sum()

# Criar um relat√≥rio dos problemas encontrados
validation_report = pd.DataFrame({
    "Coluna": df_transformed_reduced.columns,
    "Valores NaN": nan_counts.values,
    "Valores Infinitos": inf_counts.values
})

validation_report

# Verificar o balanceamento da vari√°vel alvo (Poss√≠vel Fraude)
target_distribution = df_transformed_reduced["Possivel_Fraude"].value_counts(normalize=True) * 100

# Criar um dataframe com a distribui√ß√£o das classes
target_balance_df = pd.DataFrame({
    "Classe": target_distribution.index,
    "Percentual (%)": target_distribution.values
})

target_balance_df

# Separar as features (X) e a vari√°vel alvo (y)
X_smote = df_transformed_reduced.drop(columns=["Possivel_Fraude"])
y_smote = df_transformed_reduced["Possivel_Fraude"]

# Aplicar Label Encoding na vari√°vel alvo para o SMOTE funcionar corretamente
y_smote_encoded = LabelEncoder().fit_transform(y_smote)

# Aplicar o SMOTE para balanceamento das classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_smote, y_smote_encoded)

# Criar um novo dataframe com os dados balanceados
df_trans_balanced = pd.DataFrame(X_resampled, columns=X_smote.columns)
df_trans_balanced["Possivel_Fraude"] = y_resampled  # Adicionar a vari√°vel alvo de volta

# Exibir a nova distribui√ß√£o da vari√°vel alvo ap√≥s o SMOTE
balanced_target_distribution = pd.DataFrame({
    "Classe": ["N√£o", "Sim"],
    "Quantidade": [sum(y_resampled == 0), sum(y_resampled == 1)]
})

balanced_target_distribution

"""Nessa etapa j√° tratamos as colunas com valores nulos e vazios, j√° criamos novas colunas, j√° selecionamos as colunas mais relevantes pro modelo, j√° balanceamos a vari√°vel alvo. Agora vamos buscar quais s√£o os melhores valores para os hyperparametros de cada modelo a ser testado. No nosso caso, testaremos o Random Forest e o XGBoost."""

# Definir os hiperpar√¢metros para Random Forest
param_grid_rf = {
    "n_estimators": [50, 100, 200],
    "max_depth": [None, 10, 20, 30],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4]
}

# Criar o modelo Random Forest
rf_model = RandomForestClassifier(random_state=42)

# Aplicar GridSearchCV
grid_search_rf = GridSearchCV(
    rf_model, param_grid_rf, cv=5, scoring="accuracy", n_jobs=-1, verbose=1
)

# Treinar o GridSearchCV com os dados balanceados
# X_resampled = X_train + dados ficticios; O mesmo vale para y_resampled
grid_search_rf.fit(X_resampled, y_resampled)

treinos_rf = pd.DataFrame(grid_search_rf.cv_results_)

# Acur√°cia em Treino
print(f"Acur√°cia em Treinamento: {grid_search_rf.best_score_ :.2%}")
print("")
print(f"Hiperpar√¢metros Ideais: {grid_search_rf.best_params_}")
print("")
print("Numero de treinamentos realizados: ", treinos_rf.shape[0])

# Definir os hiperpar√¢metros para XGBoost
param_grid_xgb = {
    "n_estimators": [50, 100, 200],
    "max_depth": [3, 6, 9],
    "learning_rate": [0.01, 0.1, 0.3],
    "subsample": [0.8, 1.0],
    "colsample_bytree": [0.8, 1.0]
}

# Criar o modelo XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Aplicar GridSearchCV para XGBoost
grid_search_xgb = GridSearchCV(
    xgb_model, param_grid_xgb, cv=5, scoring="accuracy", n_jobs=-1, verbose=1
)

# Treinar o GridSearchCV com os dados balanceados
# X_resampled = X_train + dados ficticios; O mesmo vale para y_resampled
grid_search_xgb.fit(X_resampled, y_resampled)

treinos_xgb = pd.DataFrame(grid_search_xgb.cv_results_)

# Acur√°cia em Treino
print(f"Acur√°cia em Treinamento: {grid_search_xgb.best_score_ :.2%}")
print("")
print(f"Hiperpar√¢metros Ideais: {grid_search_xgb.best_params_}")
print("")
print("Numero de treinamentos realizados: ", treinos_xgb.shape[0])

"""Agora que temos os melhores valores para os hyperparametros de cada modelo, vamos cri√°-los usando esses valores e test√°-los na base de teste.

Antes precisamos garantir que a base de testes est√° correta, ou seja, cont√©m somente as colunas mais relevantes e n√£o possui NAs, Vazios ou valores infinitos.
"""

X_test.head()

columns_to_keep

# Garantir que X_test e y_test est√£o definidos corretamente
X_test = X_test[columns_to_keep]

X_test.head()

# Verificar novamente se h√° valores infinitos ou extremamente grandes
problematic_values = X_test[(X_test == np.inf) | (X_test == -np.inf) | (X_test > np.finfo(np.float32).max)].any()

# Criar um relat√≥rio das colunas problem√°ticas
problematic_values_df = pd.DataFrame(problematic_values, columns=["Possui Problema"])
problematic_values_df = problematic_values_df[problematic_values_df["Possui Problema"]]

problematic_values_df

y_test

# Criar o modelo Random Forest com os melhores hiperpar√¢metros encontrados
rf_best_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=None,
    min_samples_leaf=4,
    min_samples_split=2,
    random_state=42
)

# Treinar o modelo Random Forest nos dados balanceados
rf_best_model.fit(X_resampled, y_resampled)

# Fazer previs√µes no conjunto de teste
y_pred_rf = rf_best_model.predict(X_test)

# Calcular m√©tricas de avalia√ß√£o para Random Forest
rf_metrics = {
    "Acur√°cia": accuracy_score(y_test, y_pred_rf),
    "Precis√£o": precision_score(y_test, y_pred_rf),
    "Recall": recall_score(y_test, y_pred_rf),
    "F1-Score": f1_score(y_test, y_pred_rf),
    "ROC-AUC": roc_auc_score(y_test, y_pred_rf)
}

rf_metrics_df = pd.DataFrame(rf_metrics, index=["RandomForest"])

rf_metrics_df

# Criar o modelo XGBoost com os melhores hiperpar√¢metros encontrados
xgb_best_model = XGBClassifier(
    colsample_bytree=0.8,
    learning_rate=0.1,
    max_depth=3,
    n_estimators=100,
    subsample=0.8,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Treinar o modelo XGBoost nos dados balanceados
xgb_best_model.fit(X_resampled, y_resampled)

# Fazer previs√µes no conjunto de teste
y_pred_xgb = xgb_best_model.predict(X_test)

# Calcular m√©tricas de avalia√ß√£o para XGBoost
xgb_metrics = {
    "Acur√°cia": accuracy_score(y_test, y_pred_xgb),
    "Precis√£o": precision_score(y_test, y_pred_xgb),
    "Recall": recall_score(y_test, y_pred_xgb),
    "F1-Score": f1_score(y_test, y_pred_xgb),
    "ROC-AUC": roc_auc_score(y_test, y_pred_xgb)
}

# Criar um dataframe com os resultados
xgb_metrics_df = pd.DataFrame(xgb_metrics, index=["XGBoost"])

xgb_metrics_df

"""Como as m√©tricas apresentaram valores muito bons, isso pode indicar overfitting. Vamos aplicar a valida√ß√£o cruzada para garantir que os modelos n√£o est√£o enviezados.

Agora que temos os modelos criados e treinados com as melhores configura√ß√µes de hyperparametros poss√≠veis, vamos realizar a valida√ß√£o cruzada mais uma vez (pois ela foi realizada no gridsearch para encontrarmos os melhores valores) pois agora a realizaremos no modelo final j√° treinado com os hyperparametros.
"""

# Definir n√∫mero de folds para a valida√ß√£o cruzada
cv_folds = 5

# Aplicar valida√ß√£o cruzada para Random Forest
rf_cv_scores = cross_val_score(rf_best_model, X_resampled, y_resampled, cv=cv_folds, scoring="accuracy", n_jobs=-1)
rf_cv_mean = rf_cv_scores.mean()

# Aplicar valida√ß√£o cruzada para XGBoost
xgb_cv_scores = cross_val_score(xgb_best_model, X_resampled, y_resampled, cv=cv_folds, scoring="accuracy", n_jobs=-1)
xgb_cv_mean = xgb_cv_scores.mean()

# Criar um dataframe com os resultados da valida√ß√£o cruzada
cv_results_df = pd.DataFrame({
    "Modelo": ["Random Forest", "XGBoost"],
    "M√©dia Acur√°cia (Cross-Validation)": [rf_cv_mean, xgb_cv_mean]
})

cv_results_df

# Criar um dataframe com os resultados das m√©tricas no conjunto de teste
final_results_df = pd.DataFrame({
    "Modelo": ["Random Forest", "XGBoost"],
    "Acur√°cia": [rf_metrics["Acur√°cia"], xgb_metrics["Acur√°cia"]],
    "Precis√£o": [rf_metrics["Precis√£o"], xgb_metrics["Precis√£o"]],
    "Recall": [rf_metrics["Recall"], xgb_metrics["Recall"]],
    "F1-Score": [rf_metrics["F1-Score"], xgb_metrics["F1-Score"]],
    "ROC-AUC": [rf_metrics["ROC-AUC"], xgb_metrics["ROC-AUC"]]
})

final_results_df

#Como os modelos s√£o muito similares em performance, seguiremos com o random forest por ser um modelo de mais f√°cil interpreta√ß√£o.

import pickle

def salvar_modelo(modelo, nome_arquivo):
    """
    Salva o modelo treinado em um arquivo usando pickle.
    
    Par√¢metros:
    modelo (objeto): Modelo treinado (Random Forest, XGBoost, etc.).
    nome_arquivo (str): Nome do arquivo para salvar o modelo.
    """
    with open(nome_arquivo, 'wb') as arquivo:
        pickle.dump(modelo, arquivo)
    print(f"Modelo salvo com sucesso em {nome_arquivo}")

# Exemplo de uso:
salvar_modelo(rf_best_model, "random_forest_model.pkl")
# salvar_modelo(xgb_best_model, "xgboost_model.pkl")

"""
As colunas que a base de dados nova precisa possuir para que o modelo consiga realizar a previs√£o corretamente s√£o:

1.   QT_Parcelas_Atraso
2.   QT_Total_Parcelas_Pagas
3.   QT_Dias_Atraso
4.   VL_Parcela
5.   Valor_Renda
"""

